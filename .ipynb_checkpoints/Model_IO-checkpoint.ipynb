{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9883b7-d000-42aa-ac57-c955217d55d5",
   "metadata": {},
   "source": [
    "# LangChain 的核心模块：Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b59ccb-42d8-4c71-8bb2-1784e0ad7dad",
   "metadata": {},
   "source": [
    "Model I/O 是LangChain为开发者提供的的一套面向LLM的标准化模型接口，包括模型输入(Prompts),模型输出(Output Parsers)和模型本身(Models)\n",
    "- Prompts: 模板化，动态输入\n",
    "- Models:调用的语言模型\n",
    "- Output Parser: 从模型输出中提取信息，并规范化内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5979b-413f-416e-a424-a6f5839bb68d",
   "metadata": {},
   "source": [
    "![img](./picture/IO_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb7e06-2c78-418d-8b19-456e0c26f6e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 安装LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4448654-bd41-414b-8bc1-0d35f52845b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./venv/lib/python3.9/site-packages (0.0.256)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.9/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.9/site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.9/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./venv/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./venv/lib/python3.9/site-packages (from langchain) (0.5.13)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in ./venv/lib/python3.9/site-packages (from langchain) (0.0.15)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./venv/lib/python3.9/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./venv/lib/python3.9/site-packages (from langchain) (1.25.1)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./venv/lib/python3.9/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./venv/lib/python3.9/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.9/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./venv/lib/python3.9/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.9/site-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in ./venv/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 安装\n",
    "!pip install langchain\n",
    "# 更新到最新的版本\n",
    "# !pip install -U langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98969831-c7b8-4983-9157-28549c25e6d6",
   "metadata": {},
   "source": [
    "## 模型输入 Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4f7ce-7546-4707-9c53-e0d107e89b94",
   "metadata": {},
   "source": [
    "一个语言模型的提示是用户提供一组指令或者文字描述，用于引导模型响应，帮助它理解上下文并生成相关和连贯的基于语言的输出，例如回答问题，完成对话\n",
    "- 提示模板 (Prompt Templates):参数化的模型输入\n",
    "- 示例选择器 (Example Selectors):动态选择要包含在提示中示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd0a4d-c386-4fe6-9510-fbb57f6ee408",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 提示模板(Prompt Templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e2dd4-9b35-429d-84a1-a0e975850ef1",
   "metadata": {},
   "source": [
    "Prompt Templates 提供了一种预定义，动态注入，模板无关和参数化的提示词生成方式，可以在不同语言模型之间重用的模板\n",
    "提示模板通常为一个字符串(LLMs)或者一组聊天消息(Chat Model)\n",
    "\n",
    "以下是常用的LangChain中常用模板类的继承关系\n",
    "\n",
    "```\n",
    "BasePromptTemplate --> PipelinePromptTemplate\n",
    "                       StringPromptTemplate --> PromptTemplate\n",
    "                                                FewShotPromptTemplate\n",
    "                                                FewShotPromptWithTemplates\n",
    "                       BaseChatPromptTemplate --> AutoGPTPrompt\n",
    "                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "BaseMessagePromptTemplate --> MessagesPlaceholder\n",
    "                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\n",
    "                                                                  HumanMessagePromptTemplate\n",
    "                                                                  AIMessagePromptTemplate\n",
    "                                                                  SystemMessagePromptTemplate\n",
    "\n",
    "PromptValue --> StringPromptValue\n",
    "                ChatPromptValue\n",
    "```\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b3001-85b2-4513-bf9f-bc1a37a038dc",
   "metadata": {},
   "source": [
    "#### 使用PromptTemplate生成提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e308ea9f-107c-416c-ab84-b709fdd67422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 方式一：使用from_template方法实例化模板\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\")\n",
    "# 使用format生成提示\n",
    "prompt = prompt_template.format(adjective=\"funny\",content=\"chickens\")\n",
    "\n",
    "print(prompt, type(prompt), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7229bbdf-cb2a-4f90-85db-dbce4e457788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about dog.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 方式二：使用构造函数实例化模板\n",
    "# 在实例化PromptTemplate的时候传参：input_variables 和 template\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\",\"content\"],\n",
    "    template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt = prompt_template.format(adjective=\"funny\",content=\"dog\")\n",
    "print(prompt, type(prompt), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e6066-a49e-4659-a232-9c4591a066ed",
   "metadata": {},
   "source": [
    "#### 使用PromptTemplate生成的提示词交互大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8d85435-dae2-40aa-98bd-a56bf5e518f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "result:\n",
      "1. 为什么程序员总是喜欢在夜晚工作？因为他们喜欢黑客入侵的感觉！\n",
      "\n",
      "2. 为什么程序员总是喜欢用黑色主题的编辑器？因为他们觉得黑色背景更酷，而且可以节省电量，让电脑更快！\n",
      "\n",
      "3. 为什么程序员总是喜欢用鼠标右键点击？因为他们觉得左键是用来修复问题的，而右键是用来制造问题的！\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "# 定义大模型\n",
    "llm = OpenAI(openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "             openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "             model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 定义模型\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"num\"],\n",
    "    template=\"讲{num}个关于程序员的笑话\"\n",
    ")\n",
    "\n",
    "# 发送请求并获取相应\n",
    "result = llm(prompt_template.format(num=3))\n",
    "print(type(result))\n",
    "print(f\"result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ee906-042c-4629-9931-5d58b0733f4b",
   "metadata": {},
   "source": [
    "#### 使用ChatPromptTemplate 生成适用于聊天模型的提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d60276-e486-4b1c-881b-b169d4c356a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is victor.', additional_kwargs={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, example=False), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, example=False), HumanMessage(content='what is your name?', additional_kwargs={}, example=False)]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 方式一：使用消息列表方法生成提示词\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "# 生成提示词\n",
    "prompt = prompt_template.format_messages(name=\"victor\", user_input=\"what is your name?\")\n",
    "print(prompt, type(prompt), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696f0ab2-38ef-4616-8fdb-6bb2753761ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI bot. Your name is victor.\n"
     ]
    }
   ],
   "source": [
    "print(prompt[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c70197-f8d6-4ffe-9fbd-0b8c352410fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is kunmzhao.', additional_kwargs={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, example=False), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, example=False), HumanMessage(content='what is your name?', additional_kwargs={}, example=False)]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 方式二：使用角色PromptTemplate生成提示词\n",
    "from langchain.prompts import HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "sysMessage = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "humanMessage_1 = HumanMessagePromptTemplate.from_template(\"Hello, how are you doing?\")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\"I'm doing well, thanks!\")\n",
    "humanMessage_2 = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([sysMessage, humanMessage_1, ai_message, humanMessage_2])\n",
    "# 生成提示词\n",
    "prompt = prompt_template.format_messages(name=\"kunmzhao\", user_input=\"what is your name?\")\n",
    "print(prompt, type(prompt), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf344d6-7b97-4e20-97d8-19a9973f4400",
   "metadata": {},
   "source": [
    "#### 使用ChatPromptTemplate生成的提示词交互大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e74628-e822-46de-9b1b-a162ae4a177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 第一篇文章的论点是PHP是世界上最好的语言，因为它无需逻辑和算法，只需要情绪。\n",
      "2. 第二篇文章的论点是Python是世界上最好的语言，因为它拥有美丽和简洁的特性。\n",
      "3. 第三篇文章的论点是Java是世界上最好的语言，因为它具有严谨和安全的编程信条。\n",
      "\n",
      "根据这些论点，我认为第三篇文章提出了更好的论点。原因如下：\n",
      "第三篇文章的论点强调了Java对规范和自律的重视，这是一种重要的编程价值观。它强调了在编程过程中的严谨性和安全性，这对于确保代码质量和安全性非常重要。与第一篇和第二篇文章相比，第三篇文章更加关注编程的核心原则和价值观，而不是只强调情感或美学。因此，第三篇文章提出了更好的论点。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "# 定义会话大模型\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "    openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# 定义会话 prompt\n",
    "summary_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你将获得关于同一主题的{num}篇文章（用-----------标签分隔）。首先总结每篇文章的论点。然后指出哪篇文章提出了更好的论点，并解释原因。\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = summary_template.format_messages(\n",
    "    num=3,\n",
    "    user_input='''1. [PHP是世界上最好的语言]\n",
    "PHP是世界上最好的情感派编程语言，无需逻辑和算法，只要情绪。它能被蛰伏在冰箱里的PHP大神轻易驾驭，会话结束后的感叹号也能传达对代码的热情。写PHP就像是在做披萨，不需要想那么多，只需把配料全部扔进一个碗，然后放到服务器上，热乎乎出炉的网页就好了。\n",
    "-----------\n",
    "2. [Python是世界上最好的语言]\n",
    "Python是世界上最好的拜金主义者语言。它坚信：美丽就是力量，简洁就是灵魂。Python就像是那个永远在你皱眉的那一刻扔给你言情小说的好友。只有Python，你才能够在两行代码之间感受到飘逸的花香和清新的微风。记住，这世上只有一种语言可以使用空格来领导全世界的进步，那就是Python。\n",
    "-----------\n",
    "3. [Java是世界上最好的语言]\n",
    "Java是世界上最好的德育课编程语言，它始终坚守了严谨、安全的编程信条。Java就像一个严格的老师，他不会对你怀柔，不会让你偷懒，也不会让你走捷径，但他教会你规范和自律。Java就像是那个喝咖啡也算加班费的上司，拥有对邪恶的深度厌恶和对善良的深度拥护。\n",
    "'''\n",
    ")\n",
    "\n",
    "result = chat_model(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f172c0-79f7-45e4-a3e1-05cb023c96e0",
   "metadata": {},
   "source": [
    "#### 使用FewShotPromtTemplate类生成提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b4483c9-9167-44d9-8fe0-6ee71c52281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 谁活得更久，穆罕默德·阿里还是艾伦·图灵？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：穆罕默德·阿里去世时多大了？\n",
      "中间答案：穆罕默德·阿里去世时74岁。\n",
      "追问：艾伦·图灵去世时多大了？\n",
      "中间答案：艾伦·图灵去世时41岁。\n",
      "所以最终答案是：穆罕默德·阿里\n",
      "\n",
      "\n",
      "Question: craigslist的创始人是什么时候出生的？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是craigslist的创始人？\n",
      "中间答案：Craigslist是由Craig Newmark创办的。\n",
      "追问：Craig Newmark是什么时候出生的？\n",
      "中间答案：Craig Newmark出生于1952年12月6日。\n",
      "所以最终答案是：1952年12月6日\n",
      "\n",
      "\n",
      "Question: 乔治·华盛顿的外祖父是谁？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是乔治·华盛顿的母亲？\n",
      "中间答案：乔治·华盛顿的母亲是Mary Ball Washington。\n",
      "追问：Mary Ball Washington的父亲是谁？\n",
      "中间答案：Mary Ball Washington的父亲是Joseph Ball。\n",
      "所以最终答案是：Joseph Ball\n",
      "\n",
      "\n",
      "Question: 《大白鲨》和《皇家赌场》的导演是同一个国家的吗？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是《大白鲨》的导演？\n",
      "中间答案：《大白鲨》的导演是Steven Spielberg。\n",
      "追问：Steven Spielberg来自哪里？\n",
      "中间答案：美国。\n",
      "追问：谁是《皇家赌场》的导演？\n",
      "中间答案：《皇家赌场》的导演是Martin Campbell。\n",
      "追问：Martin Campbell来自哪里？\n",
      "中间答案：新西兰。\n",
      "所以最终答案是：不是\n",
      "\n",
      "\n",
      "Question:习近平和袁隆平谁的年纪大?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# 定义例子\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"谁活得更久，穆罕默德·阿里还是艾伦·图灵？\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：穆罕默德·阿里去世时多大了？\n",
    "中间答案：穆罕默德·阿里去世时74岁。\n",
    "追问：艾伦·图灵去世时多大了？\n",
    "中间答案：艾伦·图灵去世时41岁。\n",
    "所以最终答案是：穆罕默德·阿里\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"craigslist的创始人是什么时候出生的？\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：谁是craigslist的创始人？\n",
    "中间答案：Craigslist是由Craig Newmark创办的。\n",
    "追问：Craig Newmark是什么时候出生的？\n",
    "中间答案：Craig Newmark出生于1952年12月6日。\n",
    "所以最终答案是：1952年12月6日\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"乔治·华盛顿的外祖父是谁？\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：谁是乔治·华盛顿的母亲？\n",
    "中间答案：乔治·华盛顿的母亲是Mary Ball Washington。\n",
    "追问：Mary Ball Washington的父亲是谁？\n",
    "中间答案：Mary Ball Washington的父亲是Joseph Ball。\n",
    "所以最终答案是：Joseph Ball\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"《大白鲨》和《皇家赌场》的导演是同一个国家的吗？\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：谁是《大白鲨》的导演？\n",
    "中间答案：《大白鲨》的导演是Steven Spielberg。\n",
    "追问：Steven Spielberg来自哪里？\n",
    "中间答案：美国。\n",
    "追问：谁是《皇家赌场》的导演？\n",
    "中间答案：《皇家赌场》的导演是Martin Campbell。\n",
    "追问：Martin Campbell来自哪里？\n",
    "中间答案：新西兰。\n",
    "所以最终答案是：不是\n",
    "\"\"\"\n",
    "  }\n",
    "]\n",
    "# 定义例子模板\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "# 创建一个 FewShotPromptTemplate 对象\n",
    "few_shot_prompt = FewShotPromptTemplate(examples=examples,example_prompt=example_prompt, suffix=\"Question:{input}\",input_variables=[\"input\"])\n",
    "prompt = few_shot_prompt.format(input=\"习近平和袁隆平谁的年纪大?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477fedc-0d96-4b17-9f4f-caf4b02aa115",
   "metadata": {},
   "source": [
    "#### 使用FewShotPromtTemplate类生成提示词交互大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "550447f2-5e72-4dec-8036-667090dbff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这里需要进一步的问题吗：是的。\n",
      "追问：习近平的出生日期是什么？\n",
      "中间答案：习近平出生于1953年6月15日。\n",
      "追问：袁隆平的出生日期是什么？\n",
      "中间答案：袁隆平出生于1930年9月7日。\n",
      "所以最终答案是：袁隆平\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "# 定义大模型\n",
    "llm = OpenAI(openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "             openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "             model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "result = llm(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b69dfb-bc46-452f-8429-cc063e6e7995",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 示例选择器 Example Selectors\n",
    "#### Select by similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67c357-d2c8-409d-bdfd-ad6c82c56594",
   "metadata": {},
   "source": [
    "如果有大量的参考示例，我们需要选择那些包含在实体词中。常根据某些条件或者规则来自动选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fe815d-0923-484e-bea8-36547034a783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in ./venv/lib/python3.11/site-packages (0.4.5)\n",
      "Requirement already satisfied: requests>=2.28 in ./venv/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in ./venv/lib/python3.11/site-packages (from chromadb) (1.10.12)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.2 in ./venv/lib/python3.11/site-packages (from chromadb) (0.7.2)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in ./venv/lib/python3.11/site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in ./venv/lib/python3.11/site-packages (from chromadb) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./venv/lib/python3.11/site-packages (from chromadb) (1.25.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./venv/lib/python3.11/site-packages (from chromadb) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.11/site-packages (from chromadb) (4.7.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./venv/lib/python3.11/site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.11/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.11/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.11/site-packages (from chromadb) (4.65.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.11/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./venv/lib/python3.11/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in ./venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
      "Requirement already satisfied: click>=7.0 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./venv/lib/python3.11/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a04ed3-5f2c-4f51-b9a7-6e7fc11043c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: worried\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# These are a lot of examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(\n",
    "        openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "        openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "        model_name=\"gpt-3.5-turbo\"\n",
    "    ), \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma, \n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\", \n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "print(similar_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131c9ea-b4d1-41d0-b5d5-6b5b169182fc",
   "metadata": {},
   "source": [
    "#### 使用示例选择器 Select by similarity 交互大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91cbe436-553e-4abf-97fd-bfbbdacfb512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "# 定义大模型\n",
    "llm = OpenAI(openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "             openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "             model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "result = llm(similar_prompt.format(adjective=\"worried\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20829df2-d79a-480b-a8f2-610257a94732",
   "metadata": {},
   "source": [
    "#### Select by length\n",
    "这个示例选择器根据长度选择要使用的示例。当担心构造一个将超过上下文窗口长度的提示时，这很有用。对于较长的输入，它会选择更少的例子，而对于较短的输入，它会选择更多的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e867fe3f-d63a-4496-b4f8-6bbedfa545b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: energetic\n",
      "Output: lethargic\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input:big\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt = example_prompt,\n",
    "    max_length=25,#Length is measured by the get_text_length function\n",
    ")\n",
    "length_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input:{adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "print(length_prompt.format(adjective=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb2a40-ffaa-49ce-9dfa-4ff91508ab23",
   "metadata": {},
   "source": [
    "#### 使用示例选择器 Select by length 交互大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ef7006-29d7-4a0c-bc1f-6d42de2e986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "# 定义大模型\n",
    "llm = OpenAI(openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "             openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "             model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "result = llm(length_prompt.format(adjective=\"big\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dff164-cdfa-4a75-9c33-8618146a450e",
   "metadata": {},
   "source": [
    "#### Custom example selector\n",
    "有时候我们可能需要自定义selector来满足不同业务的开发\n",
    "自定义的selector需要依赖两个方法\n",
    "- add_example: 接收example\n",
    "- select_examples: 筛选需要的example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5535fb9-9584-48c5-85cf-4ee543f6c348",
   "metadata": {},
   "source": [
    "下面自定义一个随机获取example的业务需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5796717-6e98-4fee-af1c-8064ab0c3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomExampleSelector(BaseExampleSelector):\n",
    "    \n",
    "    def __init__(self, examples: List[Dict[str, str]]):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example: Dict[str, str]) -> None:\n",
    "        \"\"\"Add new example to store for a key.\"\"\"\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
    "        return np.random.choice(self.examples, size=2, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "824874a3-c69c-44df-8356-7dc38f03a990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'happy', 'output': 'sad'},\n",
       " {'input': 'tall', 'output': 'short'},\n",
       " {'input': 'energetic', 'output': 'lethargic'},\n",
       " {'input': 'sunny', 'output': 'gloomy'},\n",
       " {'input': 'windy', 'output': 'calm'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "example_selector = CustomExampleSelector(examples=examples)\n",
    "example_selector.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a4e4118-dcad-4707-9649-a432642f105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'energetic', 'output': 'lethargic'}\n",
      " {'input': 'windy', 'output': 'calm'}]\n"
     ]
    }
   ],
   "source": [
    "print(example_selector.select_examples({\"input\":\"big\",\"output\":\"small\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba41ff21-bdbc-4b8a-a511-4c67fa17ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "input:energetic\n",
      "output:lethargic\n",
      "\n",
      "input:windy\n",
      "output:calm\n",
      "\n",
      "input:small\n",
      "output:\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\",\"output\",],\n",
    "    template=\"input:{input}\\noutput:{output}\",\n",
    ")\n",
    "\n",
    "custom_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"input:{input}\\noutput:\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "prompt = custom_prompt.format(input=\"small\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84864a-ab43-41c9-b299-19f63b144fc6",
   "metadata": {},
   "source": [
    "## 模型 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63045cbe-26fb-4ae0-8486-3cd32711ad7d",
   "metadata": {},
   "source": [
    "- 语言模型(LLMs):LangChain的核心组件，LangChain本身不提供自己的LLMs，而是与许多不同的LLMs(OpenAI,Hugging Face等)进行交互提供了一系列标准接口\n",
    "- 聊天模型(Chat Models): 语言模型的一种变体，虽然聊天模型内部使用了语言模型，但是LangChain提供的接口是不同的，提供了一个以聊天为输入和输出的接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d2812-f0de-4e46-beed-55582a193f4e",
   "metadata": {},
   "source": [
    "### 语言模型 LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b15c8-90fd-4a86-ada9-551a8c99d60d",
   "metadata": {},
   "source": [
    "\n",
    "类继承关系：\n",
    "\n",
    "```\n",
    "BaseLanguageModel --> BaseLLM --> LLM --> <name>  # Examples: AI21, HuggingFaceHub, OpenAI\n",
    "```\n",
    "\n",
    "**API 参考文档：https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.llms**\n",
    "\n",
    "LLMs 已支持模型清单\n",
    "\n",
    "**开发者文档：https://python.langchain.com/docs/integrations/llms/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b108ecc-4f0a-429d-b473-f3abd8139a30",
   "metadata": {},
   "source": [
    "以下主要介绍关于OpenAI的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4447f-25ee-481e-bdac-97a2bea8ea92",
   "metadata": {},
   "source": [
    "#### 使用 LangChain 调用 OpenAI GPT Completion API\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/openai.py**\n",
    "```python\n",
    "\n",
    "class BaseOpenAI(BaseLLM):\n",
    "    \"\"\"Base OpenAI large language model class.\"\"\"\n",
    "\n",
    "    client: Any  #: :meta private:\n",
    "    model_name: str = Field(\"text-davinci-003\", alias=\"model\")\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "    max_tokens: int = 256\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\n",
    "    -1 returns as many tokens as possible given the prompt and\n",
    "    the models maximal context size.\"\"\"\n",
    "    top_p: float = 1\n",
    "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
    "    frequency_penalty: float = 0\n",
    "    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n",
    "    presence_penalty: float = 0\n",
    "    \"\"\"Penalizes repeated tokens.\"\"\"\n",
    "    n: int = 1\n",
    "    \"\"\"How many completions to generate for each prompt.\"\"\"\n",
    "    best_of: int = 1\n",
    "    \"\"\"Generates best_of completions server-side and returns the \"best\".\"\"\"\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n",
    "    openai_api_key: Optional[str] = None\n",
    "    openai_api_base: Optional[str] = None\n",
    "    openai_organization: Optional[str] = None\n",
    "    # to support explicit proxy for OpenAI\n",
    "    openai_proxy: Optional[str] = None\n",
    "    batch_size: int = 20\n",
    "    \"\"\"Batch size to use when passing multiple documents to generate.\"\"\"\n",
    "    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n",
    "    \"\"\"Timeout for requests to OpenAI completion API. Default is 600 seconds.\"\"\"\n",
    "    logit_bias: Optional[Dict[str, float]] = Field(default_factory=dict)\n",
    "    \"\"\"Adjust the probability of specific tokens being generated.\"\"\"\n",
    "    max_retries: int = 6\n",
    "    \"\"\"Maximum number of retries to make when generating.\"\"\"\n",
    "    streaming: bool = False\n",
    "    \"\"\"Whether to stream the results or not.\"\"\"\n",
    "    allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set()\n",
    "    \"\"\"Set of special tokens that are allowed。\"\"\"\n",
    "    disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\"\n",
    "    \"\"\"Set of special tokens that are not allowed。\"\"\"\n",
    "    tiktoken_model_name: Optional[str] = None\n",
    "    \"\"\"The model name to pass to tiktoken when using this class. \n",
    "    Tiktoken is used to count the number of tokens in documents to constrain \n",
    "    them to be under a certain limit. By default, when set to None, this will \n",
    "    be the same as the embedding model name. However, there are some cases \n",
    "    where you may want to use this Embedding class with a model name not \n",
    "    supported by tiktoken. This can include when using Azure embeddings or \n",
    "    when using one of the many model providers that expose an OpenAI-like \n",
    "    API but with different models. In those cases, in order to avoid erroring \n",
    "    when tiktoken is called, you can specify a model name to use here.\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "class OpenAI(BaseOpenAI):\n",
    "    \"\"\"OpenAI large language models.\n",
    "\n",
    "    To use, you should have the ``openai`` python package installed, and the\n",
    "    environment variable ``OPENAI_API_KEY`` set with your API key.\n",
    "\n",
    "    Any parameters that are valid to be passed to the openai.create call can be passed\n",
    "    in, even if not explicitly saved on this class.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.llms import OpenAI\n",
    "            openai = OpenAI(model_name=\"text-davinci-003\")\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _invocation_params(self) -> Dict[str, Any]:\n",
    "        return {**{\"model\": self.model_name}, **super()._invocation_params}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7a92aaf-d591-4c48-972d-8e08b4a496e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，给你讲一个笑话：\n",
      "\n",
      "有一天，小明去参加一个面试。面试官问：“小明，你有什么特长吗？”小明回答：“我可以模仿动物的叫声。”面试官觉得很好奇，就说：“那好，你模仿一下猫叫吧。”小明立刻弯下腰，用力地喵喵喵地叫了几声。面试官惊讶地说：“太棒了！你还可以模仿其他动物吗？”小明点了点头，然后闭上眼睛，用力地喘了几口气，然后说：“我是一只鱼，我是一只鱼...”\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "             openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "             model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "result = llm(\"给我讲1个笑话\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "534e4a28-b7a6-4570-b5de-77d8691d203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.llms.openai.OpenAIChat'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenAIChat' object has no attribute 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(llm))\n\u001b[1;32m      2\u001b[0m llm\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAIChat' object has no attribute 'temperature'"
     ]
    }
   ],
   "source": [
    "print(type(llm))\n",
    "llm.__dict__\n",
    "llm.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4834d8db-238f-423f-b899-75ccc445f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 68\n",
      "\tPrompt Tokens: 23\n",
      "\tCompletion Tokens: 45\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $0.0001245\n",
      "===========\n",
      "===========\n",
      "Sure, here's a classic one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "Sure, here's another joke for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "# 跟踪特定呼叫的令牌使用情况。它目前只针对OpenAI API实现。\n",
    "from langchain.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    result1 = llm(\"Tell me a another joke\")\n",
    "    print(cb)\n",
    "print(\"===========\")\n",
    "\n",
    "print(\"===========\")\n",
    "print(result, result1, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a67adb-5750-4af0-ad27-dd3f2ead2a30",
   "metadata": {},
   "source": [
    "### 聊天模型 Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd7282-2a1b-42ee-afc3-e206fde461a9",
   "metadata": {},
   "source": [
    "类继承关系：\n",
    "\n",
    "```\n",
    "BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm\n",
    "```\n",
    "**API 参考文档：https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.chat_models**\n",
    "```python\n",
    "\n",
    "class BaseChatModel(BaseLanguageModel[BaseMessageChunk], ABC):\n",
    "    \"\"\"Base class for chat models.\"\"\"\n",
    "\n",
    "    cache: Optional[bool] = None\n",
    "    \"\"\"是否缓存响应。\"\"\"\n",
    "    verbose: bool = Field(default_factory=_get_verbosity)\n",
    "    \"\"\"是否打印响应文本。\"\"\"\n",
    "    callbacks: Callbacks = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的回调函数。\"\"\"\n",
    "    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的回调函数管理器。\"\"\"\n",
    "    tags: Optional[List[str]] = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的标签。\"\"\"\n",
    "    metadata: Optional[Dict[str, Any]] = Field(default=None, exclude=True)\n",
    "    \"\"\"添加到运行追踪的元数据。\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class ChatOpenAI(BaseChatModel):\n",
    "    \"\"\"Wrapper around OpenAI Chat large language models.\n",
    "\n",
    "    To use, you should have the ``openai`` python package installed, and the\n",
    "    environment variable ``OPENAI_API_KEY`` set with your API key.\n",
    "\n",
    "    Any parameters that are valid to be passed to the openai.create call can be passed\n",
    "    in, even if not explicitly saved on this class.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.chat_models import ChatOpenAI\n",
    "            openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        return {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
    "\n",
    "    @property\n",
    "    def lc_serializable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    client: Any = None  #: :meta private:\n",
    "    model_name: str = Field(default=\"gpt-3.5-turbo\", alias=\"model\")\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n",
    "    openai_api_key: Optional[str] = None\n",
    "    \"\"\"Base URL path for API requests, \n",
    "    leave blank if not using a proxy or service emulator.\"\"\"\n",
    "    openai_api_base: Optional[str] = None\n",
    "    openai_organization: Optional[str] = None\n",
    "    # to support explicit proxy for OpenAI\n",
    "    openai_proxy: Optional[str] = None\n",
    "    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n",
    "    \"\"\"Timeout for requests to OpenAI completion API. Default is 600 seconds.\"\"\"\n",
    "    max_retries: int = 6\n",
    "    \"\"\"Maximum number of retries to make when generating.\"\"\"\n",
    "    streaming: bool = False\n",
    "    \"\"\"Whether to stream the results or not.\"\"\"\n",
    "    n: int = 1\n",
    "    \"\"\"Number of chat completions to generate for each prompt.\"\"\"\n",
    "    max_tokens: Optional[int] = None\n",
    "    \"\"\"Maximum number of tokens to generate.\"\"\"\n",
    "    tiktoken_model_name: Optional[str] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc83e9af-6e2d-488e-a35f-f7df64ed202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "    openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    streaming=True, \n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518d6665-40c9-4b96-a241-9e468cab8a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at Globe Life Field in Arlington, Texas.The 2020 World Series was played at Globe Life Field in Arlington, Texas.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "messages = [SystemMessage(content=\"You are a helpful assistant.\"),\n",
    " HumanMessage(content=\"Who won the world series in 2020?\"),\n",
    " AIMessage(content=\"The Los Angeles Dodgers won the World Series in 2020.\"), \n",
    " HumanMessage(content=\"Where was it played?\")]\n",
    "\n",
    "chat_result = chat_model(messages)\n",
    "print(chat_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227d12a-d30a-44bb-800b-f5d7170e5a38",
   "metadata": {},
   "source": [
    "## 模型输出 Output Parser\n",
    "语言模型的输出是文本。\n",
    "\n",
    "但很多时候，您可能希望获得比纯文本更结构化的信息。这就是输出解析器的价值所在。\n",
    "\n",
    "输出解析器是帮助结构化语言模型响应的类。它们必须实现两种主要方法：\n",
    "\n",
    "    \"获取格式指令\"：返回一个包含有关如何格式化语言模型输出的字符串的方法。\n",
    "    \"解析\"：接受一个字符串（假设为来自语言模型的响应），并将其解析成某种结构。\n",
    "    \"使用提示进行解析\"：接受一个字符串（假设为来自语言模型的响应）和一个提示（假设为生成此响应的提示），并将其解析成某种结构。在需要重新尝试或修复输出，并且需要从提示中获取信息以执行此操作时，通常会提供提示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2086cf-befb-429b-b889-8230a8752c04",
   "metadata": {},
   "source": [
    "列表解析\n",
    "\n",
    "当您想要返回一个逗号分隔的项目列表时，可以使用此输出解析器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc08863-b4a6-43e9-924e-40814b724b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List five ice cream flavors.\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 创建一个输出解析器，用于处理带逗号分隔的列表输出\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 获取格式化指令，该指令告诉模型如何格式化其输出\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# 创建一个提示模板\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",  # 模板内容\n",
    "    input_variables=[\"subject\"],  # 输入变量\n",
    "    partial_variables={\"format_instructions\": format_instructions}  # 预定义的变量，这里我们传入格式化指令\n",
    ")\n",
    "# 使用提示模板和给定的主题来格式化输入\n",
    "_input = prompt.format(subject=\"ice cream flavors\")\n",
    "print(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd12c03-6ee7-4bbc-91bf-bbc7a3c03e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunmzhao/Desktop/project/langchain_study/venv/lib/python3.11/site-packages/langchain/llms/openai.py:201: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/kunmzhao/Desktop/project/langchain_study/venv/lib/python3.11/site-packages/langchain/llms/openai.py:786: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# 定义大模型\n",
    "llm = OpenAI(openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "             openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "             model_name=\"gpt-3.5-turbo\")\n",
    "result = llm(_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5703bee4-128c-4a4e-9f5c-ab71e2578ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla',\n",
       " 'Chocolate',\n",
       " 'Strawberry',\n",
       " 'Mint Chocolate Chip',\n",
       " 'Cookies and Cream']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用之前创建的输出解析器来解析模型的输出\n",
    "output_parser.parse(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36edcf5-c5d3-4456-9c72-9b226eebf40e",
   "metadata": {},
   "source": [
    "#### 日期解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4862b77-53f3-497a-bd0b-c81b989e943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997-07-01T00:00:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "# print(format_instructions)\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "your anwser should be like above\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = LLMChain(prompt=prompt_template, llm=OpenAI(\n",
    "    openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "    openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    "))\n",
    "\n",
    "output = chain.run(\"When did Hong Kong return to China?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b98f9d-8d19-4050-88c8-5cd1549262e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1997, 7, 1, 0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acc1ec1-bfdc-4e1c-a7ad-a0db6e149d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(output_parser.parse(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877deebc-f03a-4f26-b360-88cc7bb378b2",
   "metadata": {},
   "source": [
    "#### 结构化输出解析\n",
    "当希望返回多个字段时，可以使用他的输出解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3d0de5-0c6d-439d-9463-be62d32d55af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.prompts.base.StringPromptValue'>\n",
      "answer the users question as best as possible.\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // answer to the user's question\n",
      "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
      "}\n",
      "```\n",
      "what's the capital of france?\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 定义了想要解析的响应格式\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(name=\"source\", description=\"source used to answer the user's question, should be a website.\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "llm=OpenAI(\n",
    "    openai_api_key=\"sk-SJBPQGvGY6P31aLpks0X6s1VaR75hgPlvdF181UESvtLNMer\", \n",
    "    openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "print(type(_input),_input.to_string(), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fdf5fa2-95aa-4bbf-83f0-0197786c7c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"answer\": \"The capital of France is Paris.\",\n",
      "\t\"source\": \"https://en.wikipedia.org/wiki/Paris\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "output = llm(_input.to_string())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d24b4-ff07-4f01-a920-3c0dba204f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
